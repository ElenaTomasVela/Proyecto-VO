{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53e22777-c82c-4b71-85cb-2c50fa95f3ec",
   "metadata": {},
   "source": [
    "# **Segmentación y Tracking de células madre usando la librería DeepCell**\n",
    "\n",
    "## Realizado por Elena Tomás Vela y Tadeo Cabrera Gómez\n",
    "\n",
    "# Indice\n",
    "- Abstract\n",
    "- Introduccion\n",
    "- Seccion teórica\n",
    "  - DeepCell\n",
    "  - Segmentación\n",
    "  - Tracking\n",
    "- Seccion practica\n",
    "  - Introduccion\n",
    "  - Segmentacion\n",
    "  - Tracking\n",
    "- Conclusiones\n",
    "- Bibliografia\n",
    "- Tabla de tiempos\n",
    "\n",
    "# **1. Abstract**\n",
    "En este trabajo usaremos la librería DeepCell, basada en TensorFlow, para poder realizar segmentación y tracking de células madre\n",
    "\n",
    "# **2. Introducción**\n",
    "\n",
    "La visión por ordenador, también llamada visión informática o artificial es una rama de la ingeniería informática que trata métodos para adquirir, procesar, analizar y comprender imágenes con el fin de reproducir información numérica o simbólica de dichas imágenes que puedan ser tratados por un ordenador.\n",
    "\n",
    "Uno de los usos de la visión por ordenador, y el que se desarrollará en este trabajo, es la segmentación y seguimiento (o tracking) de células en imágenes biomédicas. La segmentación celular consiste en identificar y delimitar cada célula dentro de una imagen, separándola del fondo, mientras que en el tracking TOOOOOOOOOOOOOOOOOOOOOODOOOOOOOOOOOOOOOOOOOOOOOO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b58b858-8038-4b89-a4b3-ea766a3f4ab4",
   "metadata": {},
   "source": [
    "Pasos:\n",
    "\n",
    "### Paso 1. Cargar datos\n",
    "\n",
    "La documentación de DeepCell cita [**una fuente para descargar datos**](https://datasets.deepcell.org/).\n",
    "\n",
    "### Paso 2. Preprocesado de datos\n",
    "\n",
    "Una vez hemos cargado los datos, el siguiente paso para entrenar un modelo de Machine Learning es realizar un **preprocesado**, donde TTTTTTTOOOOOOOOOODDDDDDDDDOOOOOOOOOO\n",
    "\n",
    "- Normalización del histograma:\n",
    "- Creación de generadores de datos\n",
    "- Aplicación de transformaciones especializadas\n",
    "- Generación de datos con las transformaciones\n",
    "- Visualización de datos preprocesados\n",
    "\n",
    "### Paso 3. Creación del modelo\n",
    "\n",
    "Una vez preprocesados los datos, el siguiente paso es la creación del modelo. Para crear el modelo, hay que configurar los hiperparámetros. La librería clasifica los hiperparámetros en estas distintas secciones\n",
    "\n",
    "- **HIPERPARÁMETROS DE LA ARQUITECTURA DEL MODELO**\n",
    "    - **backbone**: Define la red neuronal base para la extracción de características.\n",
    "    - **location**: Activa el uso de información de ubicación espacial en la red. Esto puede mejorar la segmentación al permitir que la red aprenda patrones espaciales.\n",
    "    - **pyramid_levels**: Especifica los niveles de la Feature Pyramid Network (FPN). Cada nivel representa una escala diferente de la imagen, permitiendo detectar objetos de distintos tamaños.\n",
    "\n",
    "- **HIPERPARÁMETROS DE AUMENTO DE DATOS Y TRANSFORMACIONES:** Estos parámetros definen cómo se manipulan las imágenes antes de ingresarlas a la red.\n",
    "    - **seed**: Semilla aleatoria para asegurar la reproducibilidad en las transformaciones de datos.\n",
    "    - **min_objects**: Establece el mínimo número de objetos (células) requeridos en una imagen para ser considerada válida.\n",
    "    - **zoom_min**: Define el factor mínimo de zoom aleatorio aplicado a las imágenes. Por ejemplo, si una imagen tiene 256x256 píxeles, se puede escalar hasta un 75% de su tamaño original.\n",
    "    - **crop_size**: Define el tamaño de recorte de las imágenes de entrada.\n",
    "    - **outer_erosion_width**: Determina la cantidad de erosión aplicada a los bordes exteriores de los objetos segmentados.\n",
    "    - **inner_distance_alpha**: Parámetro que ajusta automáticamente la distancia interna de los objetos en la segmentación.\n",
    "    - **inner_distance_beta**: Controla la escala de la distancia interna entre objetos. Un valor mayor enfatiza la distancia entre células en la segmentación.\n",
    "    - **inner_erosion_width**: Define cuánto se erosionan los bordes internos de los objetos segmentados. 0 significa que no se aplica erosión interna.\n",
    "      \n",
    "- **HIPERPARÁMETROS DE POST PROCESADO:** Estos parámetros afectan cómo se refinan los resultados después de que el modelo hace una predicción.\n",
    "    - **maxima_threshold**: Define el umbral para identificar máximos locales en el mapa de predicción.\n",
    "    - **interior_threshold**: Umbral para definir qué partes de una célula se consideran su interior en la segmentación. Valores bajos permiten detectar células más pequeñas.\n",
    "    - **exclude_border**: Determina si se excluyen los objetos en los bordes de la imagen durante la segmentación.\n",
    "    - **small_objects_threshold**: Tamaño mínimo para considerar un objeto segmentado como válido (0 significa que no se eliminan objetos pequeños).\n",
    "    - **min_distance**: Define la distancia mínima entre objetos detectados para considerarlos como separados.\n",
    "\n",
    "- **HIPERPARÁMETROS DE ENTRENAMIENTO:** Estos parámetros controlan cómo se entrena la red neuronal.\n",
    "    - **epochs**: Número de veces que la red ve el conjunto de datos completos durante el entrenamiento.\n",
    "    - **batch_size**: Número de imágenes procesadas simultáneamente en cada paso de entrenamiento.\n",
    "    - **lr**: Tasa de aprendizaje del optimizador, es decir, cuánto ajusta los pesos en cada iteración.\n",
    "\n",
    "### Paso 4. Entrenamiento del modelo\n",
    "\n",
    "\n",
    "### Paso 5. Evaluación del modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e61e3f-37a8-47c3-9d51-edc764315472",
   "metadata": {},
   "source": [
    "Tabla de tiempos:\n",
    "- Investigación de distintas propuestas de métodos de trabajo: 30 minutos\n",
    "- Martes 25: 1 h 30 min\n",
    "- Sección teórica\n",
    "\n",
    "  - Abstract: \n",
    "  - Introducción: \n",
    "  - Segmentación: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934b9a9-9448-443f-a190-f6c851f391a7",
   "metadata": {},
   "source": [
    "Bibliografia:\n",
    "- [Cell Tracking Challenge](https://celltrackingchallenge.net/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f2b6a-fb44-4eeb-a9de-fb865712e99c",
   "metadata": {},
   "source": [
    "### Paso 1. Cargar datos\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ff4d0",
   "metadata": {},
   "source": [
    "LUNES: 30 min\n",
    "MARTES: 1h 30min\n",
    "MIERCOLES: 12:30 ~ ??:??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dac1cd3",
   "metadata": {},
   "source": [
    "### Paso 3. Creación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f47cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.feature import peak_local_max\n",
    "import tensorflow as tf\n",
    "\n",
    "from deepcell.applications import NuclearSegmentation\n",
    "from deepcell.image_generators import CroppingDataGenerator\n",
    "from deepcell.losses import weighted_categorical_crossentropy\n",
    "from deepcell.model_zoo.panopticnet import PanopticNet\n",
    "from deepcell.utils.train_utils import count_gpus, rate_scheduler\n",
    "from deepcell_toolbox.deep_watershed import deep_watershed\n",
    "from deepcell_toolbox.metrics import Metrics\n",
    "from deepcell_toolbox.processing import histogram_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2145f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/data'\n",
    "model_path = 'NuclearSegmentation'\n",
    "metrics_path = 'metrics.yaml'\n",
    "train_log = 'train_log.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "155346b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data\\\\train.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain.npz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[0;32m      2\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\tcabg\\Desktop\\Máster\\Visión por Ordenador\\Semana 4\\Proyecto-VO\\.venv\\lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data\\\\train.npz'"
     ]
    }
   ],
   "source": [
    "with np.load(os.path.join(data_dir, 'train.npz')) as data:\n",
    "    X_train = data['X']\n",
    "    y_train = data['y']\n",
    "\n",
    "with np.load(os.path.join(data_dir, 'val.npz')) as data:\n",
    "    X_val = data['X']\n",
    "    y_val = data['y']\n",
    "\n",
    "with np.load(os.path.join(data_dir, 'test.npz')) as data:\n",
    "    X_test = data['X']\n",
    "    y_test = data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5178566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "backbone = \"efficientnetv2bl\"\n",
    "location = True\n",
    "pyramid_levels = [\"P1\",\"P2\",\"P3\",\"P4\",\"P5\",\"P6\",\"P7\"]\n",
    "\n",
    "# Augmentation and transform parameters\n",
    "seed = 0\n",
    "min_objects = 1\n",
    "zoom_min = 0.75\n",
    "crop_size = 256\n",
    "outer_erosion_width = 1\n",
    "inner_distance_alpha = \"auto\"\n",
    "inner_distance_beta = 1\n",
    "inner_erosion_width = 0\n",
    "\n",
    "# Post processing parameters\n",
    "maxima_threshold = 0.1\n",
    "interior_threshold = 0.01\n",
    "exclude_border = False\n",
    "small_objects_threshold = 0\n",
    "min_distance = 10\n",
    "\n",
    "# Training configuration\n",
    "epochs = 16\n",
    "batch_size = 16\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26515263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-l_notop.h5\n",
      " 27000832/473176280 [>.............................] - ETA: 2:47"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (crop_size, crop_size, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPanopticNet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_semantic_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackbone_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpyramid_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpyramid_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tcabg\\Desktop\\Máster\\Visión por Ordenador\\Semana 4\\Proyecto-VO\\.venv\\lib\\site-packages\\deepcell\\model_zoo\\panopticnet.py:262\u001b[0m, in \u001b[0;36mPanopticNet\u001b[1;34m(backbone, input_shape, inputs, backbone_levels, pyramid_levels, create_pyramid_features, create_semantic_head, frames_per_batch, temporal_mode, num_semantic_classes, required_channels, norm_method, pooling, location, use_imagenet, lite, upsample_type, interpolation, name, z_axis_convolutions, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m fixed_input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(fixed_input_shape)\n\u001b[0;32m    255\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_top\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m'\u001b[39m: fixed_input_shape,\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpooling\u001b[39m\u001b[38;5;124m'\u001b[39m: pooling\n\u001b[0;32m    260\u001b[0m }\n\u001b[1;32m--> 262\u001b[0m _, backbone_dict \u001b[38;5;241m=\u001b[39m get_backbone(backbone, fixed_inputs,\n\u001b[0;32m    263\u001b[0m                                 use_imagenet\u001b[38;5;241m=\u001b[39muse_imagenet,\n\u001b[0;32m    264\u001b[0m                                 frames_per_batch\u001b[38;5;241m=\u001b[39mframes_per_batch,\n\u001b[0;32m    265\u001b[0m                                 return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m                                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m    268\u001b[0m backbone_dict_reduced \u001b[38;5;241m=\u001b[39m {k: backbone_dict[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m backbone_dict\n\u001b[0;32m    269\u001b[0m                          \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m backbone_levels}\n\u001b[0;32m    271\u001b[0m ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m frames_per_batch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m3\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\tcabg\\Desktop\\Máster\\Visión por Ordenador\\Semana 4\\Proyecto-VO\\.venv\\lib\\site-packages\\deepcell\\utils\\backbone_utils.py:450\u001b[0m, in \u001b[0;36mget_backbone\u001b[1;34m(backbone, input_tensor, input_shape, use_imagenet, return_dict, frames_per_batch, **kwargs)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_imagenet:\n\u001b[0;32m    449\u001b[0m     kwargs_with_weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_preprocessing\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 450\u001b[0m     model_with_weights \u001b[38;5;241m=\u001b[39m model_cls(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_with_weights)\n\u001b[0;32m    451\u001b[0m     model_with_weights\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    452\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, by_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\tcabg\\Desktop\\Máster\\Visión por Ordenador\\Semana 4\\Proyecto-VO\\.venv\\lib\\site-packages\\keras\\applications\\efficientnet_v2.py:1245\u001b[0m, in \u001b[0;36mEfficientNetV2L\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, include_preprocessing)\u001b[0m\n\u001b[0;32m   1233\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.applications.efficientnet_v2.EfficientNetV2L\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1234\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.applications.EfficientNetV2L\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mEfficientNetV2L\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1243\u001b[0m     include_preprocessing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1244\u001b[0m ):\n\u001b[1;32m-> 1245\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEfficientNetV2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m      \u001b[49m\u001b[43mwidth_coefficient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdepth_coefficient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdefault_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m480\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mefficientnetv2-l\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m      \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_top\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m      \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpooling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpooling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[43m      \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[43m      \u001b[49m\u001b[43mclassifier_activation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier_activation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1257\u001b[0m \u001b[43m      \u001b[49m\u001b[43minclude_preprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_preprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1258\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tcabg\\Desktop\\Máster\\Visión por Ordenador\\Semana 4\\Proyecto-VO\\.venv\\lib\\site-packages\\keras\\applications\\efficientnet_v2.py:1054\u001b[0m, in \u001b[0;36mEfficientNetV2\u001b[1;34m(width_coefficient, depth_coefficient, default_size, dropout_rate, drop_connect_rate, depth_divisor, min_depth, bn_momentum, activation, blocks_args, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, include_preprocessing)\u001b[0m\n\u001b[0;32m   1052\u001b[0m     file_hash \u001b[38;5;241m=\u001b[39m WEIGHTS_HASHES[model_name[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]][\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1053\u001b[0m   file_name \u001b[38;5;241m=\u001b[39m model_name \u001b[38;5;241m+\u001b[39m file_suffix\n\u001b[1;32m-> 1054\u001b[0m   weights_path \u001b[38;5;241m=\u001b[39m \u001b[43mdata_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m      \u001b[49m\u001b[43mBASE_WEIGHTS_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcache_subdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfile_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_hash\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1059\u001b[0m   model\u001b[38;5;241m.\u001b[39mload_weights(weights_path)\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tcabg\\Desktop\\Máster\\Visión por Ordenador\\Semana 4\\Proyecto-VO\\.venv\\lib\\site-packages\\keras\\utils\\data_utils.py:277\u001b[0m, in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    276\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 277\u001b[0m     \u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg\u001b[38;5;241m.\u001b[39mformat(origin, e\u001b[38;5;241m.\u001b[39mcode, e\u001b[38;5;241m.\u001b[39mmsg))\n",
      "File \u001b[1;32mc:\\Users\\tcabg\\Desktop\\Máster\\Visión por Ordenador\\Semana 4\\Proyecto-VO\\.venv\\lib\\site-packages\\keras\\utils\\data_utils.py:84\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m     82\u001b[0m response \u001b[38;5;241m=\u001b[39m urlopen(url, data)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[1;32m---> 84\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunk_read(response, reporthook\u001b[38;5;241m=\u001b[39mreporthook):\n\u001b[0;32m     85\u001b[0m     fd\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[1;32mc:\\Users\\tcabg\\Desktop\\Máster\\Visión por Ordenador\\Semana 4\\Proyecto-VO\\.venv\\lib\\site-packages\\keras\\utils\\data_utils.py:73\u001b[0m, in \u001b[0;36murlretrieve.<locals>.chunk_read\u001b[1;34m(response, chunk_size, reporthook)\u001b[0m\n\u001b[0;32m     71\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   chunk \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m   count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     75\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m reporthook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:464\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    463\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 464\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1271\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_shape = (crop_size, crop_size, 1)\n",
    "\n",
    "model = PanopticNet(\n",
    "    backbone=backbone,\n",
    "    input_shape=input_shape,\n",
    "    norm_method=None,\n",
    "    num_semantic_classes=[1, 1, 2],\n",
    "    location=location,\n",
    "    include_top=True,\n",
    "    backbone_levels=[\"C1\", \"C2\", \"C3\", \"C4\", \"C5\"],\n",
    "    pyramid_levels=pyramid_levels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d77f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_loss(n_classes):\n",
    "    def _semantic_loss(y_pred, y_true):\n",
    "        if n_classes > 1:\n",
    "            return 0.01 * weighted_categorical_crossentropy(y_pred, y_true, n_classes=n_classes)\n",
    "        return tf.keras.losses.MSE(y_pred, y_true)\n",
    "\n",
    "    return _semantic_loss\n",
    "\n",
    "loss = {}\n",
    "\n",
    "# Give losses for all of the semantic heads\n",
    "for layer in model.layers:\n",
    "    if layer.name.startswith(\"semantic_\"):\n",
    "        n_classes = layer.output_shape[-1]\n",
    "        loss[layer.name] = semantic_loss(n_classes)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=lr, clipnorm=0.001)\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
